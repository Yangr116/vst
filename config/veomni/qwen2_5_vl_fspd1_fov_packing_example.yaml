model:
  model_path: Qwen2.5-VL-3B-Instruct
  attn_implementation: 'flash_attention_2'
  max_pixels: 3211264

data:
  train_path: config/data/llavanext.yaml
  chat_template: qwen2_5vl
  max_seq_len: 16384
  fps: 2
  train_size: 20_531_761  # tokens for packing
  dataloader_type: "native"
  datasets_type: "iterable"
  data_name: "llava_sft_preprocess"
  num_workers: 8
  pin_memory: false

train:
  enable_fov: true
  fov_no_resize: false
  output_dir: work_dirs/qwen2_5vl_sft_llavanext_example
  enable_gradient_checkpointing: true
  enable_mixed_precision: true
  data_parallel_mode: fsdp1
  enable_full_shard: true
  # enable_fsdp_offload: true
  # enable_activation_offload: true
  wandb_project: veomni_qwen2vl_sft
  wandb_name: qwen2_5vl_sft_llavanext_example
  rmpad: false
  rmpad_with_pos_ids: true
  ulysses_parallel_size: 1
  freeze_vit: false
  freeze_llm: false
  freeze_merger: false
  lr: 5.0e-5
  vit_lr: 5.0e-6
  lr_warmup_ratio: 0.1
  lr_decay_style: cosine
  num_train_epochs: 1
  micro_batch_size: 2
  global_batch_size: 128
  save_steps: 500
  save_hf_weights: false
  empty_cache_steps: 100
  ckpt_manager: dcp
