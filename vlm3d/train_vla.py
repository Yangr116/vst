# Copyright 2025 [Visual Spatial Tuning] Authors
# This file may have been modified by [Visual Spatial Tuning] Authors
# source file: https://github.com/ByteDance-Seed/VeOmni/blob/main/tasks/train_torch.py

import json
import os
import time
from dataclasses import asdict, dataclass, field
from functools import partial
from io import BytesIO
from typing import TYPE_CHECKING, Any, Callable, Dict, List, Optional

from PIL import PngImagePlugin, Image, ImageFile
MaximumDecompressedSize = 1024
MegaByte = 2**20
PngImagePlugin.MAX_TEXT_CHUNK = MaximumDecompressedSize * MegaByte
Image.MAX_IMAGE_PIXELS = None
ImageFile.LOAD_TRUNCATED_IMAGES = True

import torch
import torch.distributed as dist
import wandb
from tqdm import trange
import yaml
import copy

try:
    import veomni
except:
    import sys
    sys.path.append(os.path.abspath('../third_party/VeOmni'))
    print(f"import veomni from {sys.path}")

from veomni.checkpoint import build_checkpointer, ckpt_to_state_dict
from veomni.data import (
    OmniDataCollatorWithPacking,
    OmniDataCollatorWithPadding,
    OmniSequenceShardCollator,
    build_byted_dataset,
    build_dataloader,
    build_mapping_dataset,
    build_multimodal_chat_template,
    build_multisource_dataset,
    build_streaming_dataloader
)
from veomni.distributed.offloading import build_activation_offloading_context
from veomni.distributed.parallel_state import get_parallel_state, init_parallel_state
from veomni.distributed.torch_parallelize import build_parallelize_model
from veomni.models import build_foundation_model, build_processor, save_model_assets, save_model_weights
from veomni.optim import build_lr_scheduler, build_optimizer
from veomni.utils import helper
from veomni.utils.arguments import DataArguments, ModelArguments, TrainingArguments, parse_args, save_args
from veomni.utils.dist_utils import all_reduce
from veomni.models.transformers.qwen2_vl.modeling_qwen2_vl import Qwen2VLForConditionalGeneration
from veomni.models.transformers.qwen2_5vl.modeling_qwen2_5_vl import Qwen2_5_VLForConditionalGeneration

if TYPE_CHECKING:
    from transformers import ProcessorMixin

    from veomni.data.chat_template import ChatTemplate

from vlm3d.utils.utils_model import print_model_params
from vlm3d.utils.data_utils import sample_data
from vlm3d.preprocess import SampleTransform
from vlm3d.load import build_vlm3d_model
from vlm3d.dataset_iterative import build_iterative_dataset
from vlm3d.chat_template import Qwen2_5VLChatTemplate
from vlm3d.constant import QWEN_IMAGE_INPUT_INDEX, QWEN_VIDEO_INPUT_INDEX
from vlm3d.vla.action_tokenizer import ActionTokenizer, ActionTokenizerV2
from vlm3d.preprocess_vla import SampleTransformVLA

from transformers import AddedToken, Qwen2VLProcessor, Qwen2_5_VLProcessor
import math


logger = helper.create_logger(__name__)

MAX_PIXELS = 4096 * 28 * 28
MIN_PIXELS = 56 * 56


def get_param_groups(model: "torch.nn.Module", default_lr: float, vit_lr: float):
    vit_params, other_params = [], []
    for name, param in model.named_parameters():
        if param.requires_grad:
            if "visual" in name:
                vit_params.append(param)
            else:
                other_params.append(param)
    param_groups = []
    if vit_params:
        param_groups.append({"params": vit_params, "lr": vit_lr})
    if other_params:
        param_groups.append({"params": other_params, "lr": default_lr})
    return param_groups


@dataclass
class MyTrainingArguments(TrainingArguments):
    freeze_vit: bool = field(
        default=False,
        metadata={"help": "Whether or not to freeze the vit parameters."},
    )
    vit_lr: float = field(
        default=1e-6,
        metadata={"help": "Maximum learning rate for vit parameters."},
    )
    freeze_llm: bool = field(
        default=False,
        metadata={"help": "Whether or not to freeze the lmm parameters."},
    )
    freeze_merger: bool = field(
        default=False,
        metadata={"help": "Whether or not to freeze the merger parameters."},
    )
    enable_fov: bool = field(
        default=False,
        metadata={"help": "Unified fov in bbox3d data"},
    )
    fov_no_resize: bool = field(
        default=False,
        metadata={"help": "Change to fov prompt, but doesn't resize the image"},
    )
    enable_uvd: bool = field(
        default=False,
        metadata={"help": "transform xyz to uvd"},
    )
    enable_predict_fov: bool = field(
        default=False,
        metadata={"help": "predict fov and bbox_3d"},
    )
    include_system: bool = field(
        default=True,
        metadata={"help": "include the system prompt"},
    )
    is_warmup: bool = field(
        default=False,
        metadata={"help": "pretrain"},
    )
    beta1: float = field(
        default=0.9
    )
    beta2: float = field(
        default=0.999
    )
    enable_quat: bool = field(
        default=False,
    )
    remove_intrinsics: bool = field(
        default=False,
    )


    def compute_train_steps(
        self, max_seq_len: Optional[int] = None, train_size: Optional[int] = None, dataset_length: Optional[int] = None
    ) -> None:
        """
        Computes the training steps per epoch according to the data length.
        """
        if self.rmpad or self.rmpad_with_pos_ids:
            assert max_seq_len is not None and train_size is not None, "max_seq_len and train_size are required."
            token_micro_bsz = self.micro_batch_size * max_seq_len
            train_size = int(train_size * (1 + self.bsz_warmup_ratio / 2))
            eff_token_rate = (token_micro_bsz - self.dyn_bsz_margin) / token_micro_bsz
            self._train_steps = math.ceil(train_size / (self.global_batch_size * max_seq_len * eff_token_rate))
        elif dataset_length is not None:
            self._train_steps = math.floor(dataset_length / self.dataloader_batch_size)  # assuming drop_last is true
            logger.info_rank0("******************")
            logger.info_rank0(f"_train_steps: {self._train_steps} dataset_length: {dataset_length} dataloader_batch_size: {self.dataloader_batch_size}")
        elif self.max_steps is not None:
            self._train_steps = self.max_steps
        else:
            raise ValueError("Please provide `dataset_length` or `max_steps`!")


@dataclass
class MyDataArguments(DataArguments):
    prefetch_factor: Optional[int] = field(
        default=None,
        metadata={"help": "Number of batches loaded in advance by each worker."},
    )
    dataset_length: Optional[int] = field(
        default=10000,
        metadata={"help": "The length of dataset, used in iterative dataset"},
    )
    fps: Optional[int] = field(
        default=4,
        metadata={"help": "fps in video data"},
    )
    buffer_size: int = field(
        default=10_000,
    )


@dataclass
class MyModelArguments(ModelArguments):
    max_pixels: Optional[int] = field(
        default=MAX_PIXELS,
        metadata={"help": "expectied max pixels."},
    )
    min_pixels: Optional[int] = field(
        default=MIN_PIXELS,
        metadata={"help": "expectied max pixels."},
    )
    vlm3d: Optional[bool] = field(
        default=False, 
        metadata={"help": "Does't continue training from qwenvl model"},
    )
    add_tokens: Optional[list[str]]=field(
        default=None,
        metadata={"help": "tokenizer added tokens"},
    )
    enable_vla: Optional[bool] = field(
        default=False, 
        metadata={"help": "Does't continue training from qwenvl model"},
    )


@dataclass
class Arguments:
    model: "ModelArguments" = field(default_factory=MyModelArguments)
    data: "MyDataArguments" = field(default_factory=MyDataArguments)
    train: "MyTrainingArguments" = field(default_factory=MyTrainingArguments)


def save_hf_ckpt(args, save_checkpoint_path, model_assets):    
    if args.train.save_hf_weights and save_checkpoint_path is not None:
        hf_weights_path = os.path.join(save_checkpoint_path, "hf_ckpt")
        model_state_dict = ckpt_to_state_dict(
            save_checkpoint_path=save_checkpoint_path,
            output_dir=args.train.output_dir,
            ckpt_manager=args.train.ckpt_manager,
        )
        save_model_weights(hf_weights_path, model_state_dict, model_assets=model_assets)
        logger.info_rank0(f"Huggingface checkpoint saved at {hf_weights_path} successfully!")


def save_ckpt(args, model, optimizer, lr_scheduler, train_dataloader, environ_meter, global_step, Checkpointer, train_metrics, model_assets):
    save_checkpoint_path = os.path.join(args.train.save_checkpoint_path, f"global_step_{global_step}")
    state = {
        "model": model,
        "optimizer": optimizer,
        "extra_state": {
            "global_step": global_step,
            "lr_scheduler": lr_scheduler.state_dict(),
            "train_dataloader": train_dataloader.state_dict(),
            "environ_meter": environ_meter.state_dict(),
        },
    }
    Checkpointer.save(args.train.save_checkpoint_path, state, global_steps=global_step)
    if args.train.global_rank == 0:
        helper.save_step2token(
            args.train.step2token_path,
            consumed_tokens=train_metrics["consume_tokens(B)"],
            global_step=global_step,
            save_checkpoint_path=save_checkpoint_path,
        )
    if args.train.global_rank == 0:
        save_hf_ckpt(args, save_checkpoint_path, model_assets)
    dist.barrier()
    logger.info_rank0(f"Distributed checkpoint saved at {save_checkpoint_path} successfully!")


def main():
    args = parse_args(Arguments)
    logger.info(f"Process rank: {args.train.global_rank}, world size: {args.train.world_size}")
    logger.info_rank0(f"\033[93m{json.dumps(asdict(args), indent=2)}\033[0m")
    torch.cuda.set_device(f"cuda:{args.train.local_rank}")
    dist.init_process_group(backend="nccl")
    seed = args.train.seed + args.train.global_rank
    helper.set_seed(seed, args.train.enable_full_determinism)
    if args.train.local_rank == 0:
        helper.enable_third_party_logging()

    if args.train.global_rank == 0:
        save_args(args, args.train.output_dir)

    Checkpointer = build_checkpointer(dist_backend=args.train.data_parallel_mode, ckpt_manager=args.train.ckpt_manager)

    init_parallel_state(
        dp_size=args.train.data_parallel_size,
        tp_size=args.train.tensor_parallel_size,
        ep_size=args.train.expert_parallel_size,
        pp_size=args.train.pipeline_parallel_size,
        cp_size=args.train.context_parallel_size,
        ulysses_size=args.train.ulysses_parallel_size,
        dp_mode=args.train.data_parallel_mode,
    )

    logger.info_rank0("Prepare model")
    if args.model.vlm3d:
        model = build_vlm3d_model(
            config_path=args.model.config_path,
            weights_path=args.model.model_path,
            encoders=args.model.encoders,
            attn_implementation=args.model.attn_implementation,
            init_device="cpu" if args.train.enable_rank0_init else "cuda")
    else:
        model = build_foundation_model(
            config_path=args.model.config_path,
            weights_path=args.model.model_path,
            init_device="cpu" if args.train.enable_rank0_init else "cuda",
            attn_implementation=args.model.attn_implementation
        )

    model_config = model.config
    helper.print_device_mem_info("VRAM usage after building model")

    logger.info_rank0("Prepare data")
    if args.data.train_path.endswith('.yaml'):
        # data_paths = yaml.safe_load(open(args.data.train_path, 'r'))
        # data_paths = [os.path.join(x['data_dir'], x['ann_path']) for x in data_paths]
        data_paths = sample_data(args.data.train_path)
        dist.broadcast_object_list(data_paths, src=0)  # all rank should get the same data, then shard them
        logger.info(f"trained files: {data_paths}")
        args.data.train_path = ",".join(data_paths)
        args.data.enable_multisource = False
    
    processor = build_processor(args.model.tokenizer_path)
    if not isinstance(processor, Qwen2VLProcessor) and not isinstance(processor, Qwen2_5_VLProcessor):
        processor = build_processor(args.model.encoders["image"]["model_path"])
    processor.image_processor.max_pixels = args.model.max_pixels
    processor.image_processor.min_pixels = args.model.min_pixels
    tokenizer = processor.tokenizer
    tokenizer.model_max_length = args.data.max_seq_len
    if args.model.add_tokens is not None:
        new_tokens = [f"<|action_{i}|>" for i in range(256)]
        logger.info(f"Add new toknes: {new_tokens}")
        num_added_tokens = tokenizer.add_tokens(new_tokens)
        if len(tokenizer) > model.model.embed_tokens.weight.shape[0]:
            model.resize_token_embeddings(len(tokenizer))
    
    processor.tokenizer = tokenizer

    position_id_func = model.get_position_id_func()
    # chat_template = build_multimodal_chat_template(args.data.chat_template, processor.tokenizer)
    chat_template = Qwen2_5VLChatTemplate(processor.tokenizer)
    assert not (args.train.enable_fov and args.train.enable_predict_fov), "enable_fov and enable_predict_fov cannot be both True"

    if args.model.enable_vla:
        action_tokenizer = ActionTokenizerV2(tokenizer=processor.tokenizer) if args.model.add_tokens is not  None else ActionTokenizer(tokenizer=processor.tokenizer)
        transform = SampleTransformVLA(
            processor=processor,
            chat_template=chat_template,
            position_id_func=position_id_func,
            action_tokenizer=action_tokenizer,
            max_seq_len=args.data.max_seq_len,
        )
    else:
        transform = SampleTransform(
            processor=processor, 
            chat_template=chat_template, 
            position_id_func=position_id_func, 
            enable_fov=args.train.enable_fov,
            no_resize=args.train.fov_no_resize,
            enable_uvd=args.train.enable_uvd,
            include_system=args.train.include_system,
            enable_quat=args.train.enable_quat,
            remove_intrinsics=args.train.remove_intrinsics,
            max_seq_len=args.data.max_seq_len,
            fps=args.data.fps,
            is_warmup=args.train.is_warmup,
            enable_predict_fov=args.train.enable_predict_fov
            )

    if args.train.rmpad:
        raise ValueError("Qwen2-VL does not support rmpad. Use `rmpad_with_pos_ids` instead.")

    data_collate_fn = []
    if args.train.rmpad_with_pos_ids:
        data_collate_fn.append(OmniDataCollatorWithPacking())
    else:
        data_collate_fn.append(OmniDataCollatorWithPadding())
    if get_parallel_state().sp_enabled:
        data_collate_fn.append(
            OmniSequenceShardCollator(
                padding_scale={
                    "pixel_values": processor.image_processor.merge_size**2,
                },
                rmpad_with_pos_ids=args.train.rmpad_with_pos_ids,
            )
        )
    
    if args.data.dataloader_type == "native":
        if args.data.datasets_type == "mapping":
            train_dataset = build_mapping_dataset(args.data.train_path, transform=transform)
            args.train.compute_train_steps(args.data.max_seq_len, args.data.train_size, len(train_dataset) // args.train.world_size)  # native dataset has some bug
        elif args.data.datasets_type == "iterable":
            logger.info(f"### set buffer size to {args.data.buffer_size}")
            train_dataset = build_iterative_dataset(args.data.train_path, transform=transform, seed=args.train.seed, buffer_size=args.data.buffer_size)
            args.train.compute_train_steps(args.data.max_seq_len, args.data.train_size, args.data.dataset_length // args.train.world_size)
        else:
            raise NotImplementedError
        
        train_dataloader = build_dataloader(
            dataset=train_dataset,
            micro_batch_size=args.train.micro_batch_size,
            global_batch_size=args.train.global_batch_size,
            dataloader_batch_size=args.train.dataloader_batch_size,
            seed=args.train.seed,
            collate_fn=data_collate_fn,
            max_seq_len=args.data.max_seq_len,
            train_steps=args.train.train_steps,
            rmpad=args.train.rmpad,
            rmpad_with_pos_ids=args.train.rmpad_with_pos_ids,
            bsz_warmup_ratio=args.train.bsz_warmup_ratio,
            dyn_bsz_margin=args.train.dyn_bsz_margin,
            dyn_bsz_buffer_size=args.train.dyn_bsz_buffer_size,
            num_workers=args.data.num_workers,
            drop_last=args.data.drop_last,
            pin_memory=args.data.pin_memory if args.data.num_workers > 0 else False,
            prefetch_factor=args.data.prefetch_factor,
        )
    elif args.data.dataloader_type == "streaming":
        if args.data.enable_multisource:
            train_dataset = build_multisource_dataset(
                data_path=args.data.train_path,
                dataloader_batch_size=args.train.dataloader_batch_size,
                max_seq_len=args.data.max_seq_len,
                transform=transform,
                shuffle=True,
                shuffle_shard_nums=args.data.shuffle_shard_nums,
                prefetch_factor=args.data.prefetch_factor,
                num_workers=args.data.num_workers,
                predownload_factor=args.data.predownload_factor,
                silent_exception=args.data.silent_exception,
            )
            args.train.compute_train_steps(args.data.max_seq_len, args.data.train_size)
        else:
            train_dataset = build_byted_dataset(
                data_path=args.data.train_path,
                dataloader_batch_size=args.train.dataloader_batch_size,
                transform=transform,
                shuffle=True,
                shuffle_seed=args.train.seed,
                shuffle_shard_nums=args.data.shuffle_shard_nums,
                split_nums=args.data.split_nums,
                predownload_factor=args.data.predownload_factor,
                silent_exception=args.data.silent_exception,
            )
            args.train.compute_train_steps(args.data.max_seq_len, args.data.train_size, len(train_dataset))
        train_dataloader = build_streaming_dataloader(
            dataset=train_dataset,
            micro_batch_size=args.train.micro_batch_size,
            global_batch_size=args.train.global_batch_size,
            dataloader_batch_size=args.train.dataloader_batch_size,
            max_seq_len=args.data.max_seq_len,
            train_steps=args.train.train_steps,
            rmpad=args.train.rmpad,
            rmpad_with_pos_ids=args.train.rmpad_with_pos_ids,
            bsz_warmup_ratio=args.train.bsz_warmup_ratio,
            dyn_bsz_runtime=args.train.dyn_bsz_runtime,
            dyn_bsz_margin=args.train.dyn_bsz_margin,
            dyn_bsz_buffer_size=args.train.dyn_bsz_buffer_size,
            collate_fn=data_collate_fn,
            num_workers=args.data.num_workers,
            drop_last=args.data.drop_last,
            pin_memory=args.data.pin_memory,
            prefetch_factor=args.data.prefetch_factor,
        )
    else:
        raise NotImplementedError(f"Unsupported dataloader type: {args.data.dataloader_type}.")

    freeze_any = False
    if args.train.freeze_vit:
        model.visual.requires_grad_(False)
        model.visual.merger.requires_grad_(True)
        freeze_any = True
    if args.train.freeze_merger:
        model.visual.merger.requires_grad_(False)
        freeze_any = True
    if args.train.freeze_llm:
        model.model.requires_grad_(False)
        freeze_any = True

    fsdp_kwargs = {}
    if freeze_any and args.train.data_parallel_mode == "fsdp1":
        fsdp_kwargs["use_orig_params"] = True

    if args.train.enable_gradient_checkpointing or args.model.add_tokens is not None:
        logger.info_rank0(f"Set enable_input_require_grads = True")
        if hasattr(model, "enable_input_require_grads"):
            model.enable_input_require_grads()
        else:

            def make_inputs_require_grad(module, input, output):
                output.requires_grad_(True)

            model.get_input_embeddings().register_forward_hook(make_inputs_require_grad)

    print_model_params(model)

    paral_args = {}
    if os.getenv('DEBUG') == '1' or args.train.data_parallel_mode == "ddp":
        paral_args = {"init_device": "cuda"}

    model = build_parallelize_model(
        model,
        enable_full_shard=args.train.enable_full_shard,
        enable_mixed_precision=args.train.enable_mixed_precision,
        enable_gradient_checkpointing=args.train.enable_gradient_checkpointing,
        enable_rank0_init=args.train.enable_rank0_init,
        enable_fsdp_offload=args.train.enable_fsdp_offload,
        fsdp_kwargs=fsdp_kwargs,
        basic_modules=model._no_split_modules,
        enable_reentrant=args.train.enable_reentrant,
        enable_forward_prefetch=args.train.enable_forward_prefetch,
        **paral_args,
    )
    optimizer = build_optimizer(
        model,
        lr=args.train.lr,
        betas=(args.train.beta1, args.train.beta2),
        weight_decay=args.train.weight_decay,
        fused=False,
        optimizer_type=args.train.optimizer,
        param_groups=get_param_groups(model, args.train.lr, args.train.vit_lr),
    )
    
    scheduler_train_steps = args.train.train_steps * args.train.num_train_epochs if args.train.max_steps is None else args.train.max_steps
    lr_scheduler = build_lr_scheduler(
        optimizer,
        train_steps=scheduler_train_steps,
        lr=args.train.lr,
        lr_min=args.train.lr_min,
        lr_decay_style=args.train.lr_decay_style,
        lr_decay_ratio=args.train.lr_decay_ratio,
        lr_warmup_ratio=args.train.lr_warmup_ratio,
        lr_start=args.train.lr_start,
    )
    # hack
    _model_config = copy.deepcopy(model_config)
    if _model_config.image_token_id != QWEN_IMAGE_INPUT_INDEX:
        _model_config.image_token_id = QWEN_IMAGE_INPUT_INDEX
    if _model_config.video_token_id != QWEN_VIDEO_INPUT_INDEX:
        _model_config.video_token_id = QWEN_VIDEO_INPUT_INDEX
    _model_config._attn_implementation_autoset = False
    model_assets = [_model_config, processor]
    
    if args.train.global_rank == 0:
        if args.train.use_wandb:
            wandb.init(
                project=args.train.wandb_project,
                name=args.train.wandb_name,
                config={**vars(args.model), **vars(args.data), **vars(args.train)},  # flatten dict
            )

        if args.train.enable_profiling:
            profiler = helper.create_profiler(
                start_step=args.train.profile_start_step,
                end_step=args.train.profile_end_step,
                trace_dir=args.train.profile_trace_dir,
                record_shapes=args.train.profile_record_shapes,
                profile_memory=args.train.profile_profile_memory,
                with_stack=args.train.profile_with_stack,
            )
            profiler.start()

        save_model_assets(args.train.model_assets_dir, model_assets)

    start_epoch, start_step, global_step = 0, 0, 0
    save_checkpoint_path = None
    environ_meter = helper.EnvironMeter(
        config=model_config,
        global_batch_size=args.train.global_batch_size,
        rmpad=args.train.rmpad,
        rmpad_with_pos_ids=args.train.rmpad_with_pos_ids,
        enable_multisource=args.data.enable_multisource,
        dataloader=train_dataloader,
        data_path=args.data.train_path,
        empty_cache_steps=args.train.empty_cache_steps,
    )
    
    if args.train.load_checkpoint_path:
        state = {"model": model, "optimizer": optimizer, "extra_state": {}}  # cannot be None
        Checkpointer.load(args.train.load_checkpoint_path, state)
        global_step = state["extra_state"]["global_step"]
        start_epoch = global_step // args.train.train_steps
        start_step = global_step % args.train.train_steps
        lr_scheduler.load_state_dict(state["extra_state"]["lr_scheduler"])
        train_dataloader.load_state_dict(state["extra_state"]["train_dataloader"])
        environ_meter.load_state_dict(state["extra_state"]["environ_meter"])
        if start_step == 0:  # resume at the end of epoch
            iter(train_dataloader)  # clear resume state and prefetch data

        if args.train.global_rank == 0:
            helper.load_step2token(args.train.load_checkpoint_path)
        dist.barrier()
        logger.info_rank0(f"Load distributed checkpoint from {args.train.load_checkpoint_path} successfully!")

    helper.empty_cache()
    model_fwd_context, model_bwd_context = build_activation_offloading_context(
        args.train.enable_activation_offload, args.train.enable_gradient_checkpointing, args.train.activation_gpu_limit
    )
    model.train()
    
    logger.info_rank0("Start training")
    for epoch in range(start_epoch, args.train.num_train_epochs):

        if hasattr(train_dataloader, "set_epoch"):
            train_dataloader.set_epoch(epoch)
        
        if args.train.rmpad_with_pos_ids and hasattr(train_dataloader, '_dataloader'):
            train_dataloader._dataloader.set_epoch(epoch)
        
        data_loader_tqdm = trange(
            args.train.train_steps,
            desc=f"Epoch {epoch + 1}/{args.train.num_train_epochs}",
            total=args.train.train_steps,
            initial=start_step,
            disable=args.train.local_rank != 0,
        )
        data_iterator = iter(train_dataloader)
        for _ in range(start_step, args.train.train_steps):
            global_step += 1
            try:
                micro_batches: List[Dict[str, Any]] = next(data_iterator)
            except StopIteration:
                logger.info(f"epoch:{epoch} Dataloader finished with drop_last {args.data.drop_last}")
                break

            if global_step == 1:
                helper.print_example(example=micro_batches[0], rank=args.train.local_rank)

            total_loss = 0
            total_action_acc = 0
            total_action_l1_loss = 0
            torch.cuda.synchronize()
            start_time = time.time()
            for micro_batch in micro_batches:
                environ_meter.add(micro_batch)
                if args.data.enable_multisource:
                    micro_batch.pop("ds_idx", None)
                    micro_batch.pop("cur_token_num", None)

                micro_batch = {
                    k: v.cuda(non_blocking=True) if isinstance(v, torch.Tensor) else v for k, v in micro_batch.items()
                }

                with model_fwd_context:
                    output = model(**micro_batch, use_cache=False)
                    loss: "torch.Tensor" = output.loss / len(micro_batches)

                with model_bwd_context:
                    loss.backward()
                
                total_loss += loss.item()

                # # TODO: VLA l1 loss visualization
                # action_logits = output.logits
                # action_preds = action_logits.argmax(dim=2)
                # action_gt = micro_batch["labels"][:, 1:].to(action_preds.device)
                # mask = action_gt > action_tokenizer.action_token_begin_idx

                # correct_preds = (action_preds == action_gt) & mask
                # action_accuracy = correct_preds.sum().float() / mask.sum().float()
                # action_accuracy = action_accuracy / len(micro_batches)
                # total_action_acc += action_accuracy.item()

                # continuous_actions_pred = torch.tensor(
                #     action_tokenizer.decode_token_ids_to_actions(action_preds[mask].cpu().numpy())
                # )
                # continuous_actions_gt = torch.tensor(
                #     action_tokenizer.decode_token_ids_to_actions(action_gt[mask].cpu().numpy())
                # )
                # action_l1_loss = torch.nn.functional.l1_loss(continuous_actions_pred, continuous_actions_gt)
                # action_l1_loss = action_l1_loss / len(micro_batches)
                # total_action_l1_loss += action_l1_loss.item()

                del micro_batch

            if args.train.data_parallel_mode == "fsdp1":
                grad_norm = model.clip_grad_norm_(args.train.max_grad_norm).item()
            else:
                grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.train.max_grad_norm, foreach=True)

            optimizer.step()
            lr_scheduler.step()
            optimizer.zero_grad()
            if hasattr(grad_norm, "full_tensor"):
                grad_norm = grad_norm.full_tensor().item()

            # collect mean loss across data parallel group
            total_loss, grad_norm, total_action_acc, total_action_l1_loss, = all_reduce((total_loss, grad_norm, total_action_acc, total_action_l1_loss), group=get_parallel_state().fsdp_group)
            torch.cuda.synchronize()
            delta_time = time.time() - start_time
            lr = max(lr_scheduler.get_last_lr())
            train_metrics = environ_meter.step(delta_time, global_step=global_step)

            data_loader_tqdm.set_postfix_str(f"loss: {total_loss:.2f}, grad_norm: {grad_norm:.2f}, lr: {lr:.2e}")
            data_loader_tqdm.update()

            if args.train.global_rank == 0:
                if args.train.use_wandb:
                    train_metrics.update(
                        {
                            "training/loss": total_loss, 
                            "training/action_accuracy": total_action_acc,
                            "training/action_l1_loss": total_action_l1_loss, 
                            "training/grad_norm": grad_norm, 
                            "training/lr": lr
                            }
                    )
                    wandb.log(train_metrics, step=global_step)

                if args.train.enable_profiling and global_step <= args.train.profile_end_step:
                    profiler.step()
                    if global_step == args.train.profile_end_step:
                        profiler.stop()

            if args.train.save_steps and global_step % args.train.save_steps == 0:
                helper.empty_cache()
                save_ckpt(args, model, optimizer, lr_scheduler, train_dataloader, environ_meter, global_step, Checkpointer, train_metrics, model_assets)

        data_loader_tqdm.close()
        start_step = 0
        helper.print_device_mem_info(f"VRAM usage after epoch {epoch+1}")
        if args.train.save_epochs and (epoch + 1) % args.train.save_epochs == 0:
            helper.empty_cache()
            save_ckpt(args, model, optimizer, lr_scheduler, train_dataloader, environ_meter, global_step, Checkpointer, train_metrics, model_assets)
        
        if global_step + 1 >= scheduler_train_steps:
            logger.info(f"Done!")
            break

    torch.cuda.synchronize()
    # release memory
    del optimizer, lr_scheduler
    helper.empty_cache()
    # save model in huggingface's format
    args.train.save_hf_weights = True
    if args.train.global_rank == 0:
        save_checkpoint_path = os.path.join(args.train.save_checkpoint_path, f"global_step_{global_step}")
        save_hf_ckpt(args, save_checkpoint_path, model_assets)
    
    dist.barrier()
    dist.destroy_process_group()

if __name__ == "__main__":
    main()
